{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "variational_autoencoder_tensorboard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUbkX9RuMVuPPTAGXFH9qe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lovellbrian/ELEC4630/blob/master/variational_autoencoder_tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo29OC5Gz7Zk",
        "colab_type": "text"
      },
      "source": [
        "#ELEC4630\n",
        "\n",
        "#Example of VAE on MNIST dataset using MLP\n",
        "\n",
        "The VAE has a modular design. The encoder, decoder and VAE\n",
        "are 3 models that share weights. After training the VAE model,\n",
        "the encoder can be used to generate latent vectors.\n",
        "The decoder can be used to generate MNIST digits by sampling the\n",
        "latent vector from a Gaussian distribution with mean = 0 and std = 1.\n",
        "\n",
        "Tip: Right click on Tensorboard and select This Frame/Open Frame in New Tab to see tensorboard in a separate window. \n",
        "\n",
        "# Reference\n",
        "\n",
        "[1] Kingma, Diederik P., and Max Welling.\n",
        "\"Auto-Encoding Variational Bayes.\"\n",
        "https://arxiv.org/abs/1312.6114\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2IhvZA1vRJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorflow 1.x compatibility \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBefO7gkyA00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gSrD4siQ7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# from layers import Lambda, Input, Dense\n",
        "# from keras.models import Model\n",
        "# from keras.datasets import mnist\n",
        "# from keras.losses import mse, binary_crossentropy\n",
        "# from keras.utils import plot_model\n",
        "\n",
        "from tensorflow.compat.v1.keras.layers import Lambda, Input, Dense\n",
        "from tensorflow.compat.v1.keras.models import Model\n",
        "from tensorflow.compat.v1.keras.datasets import mnist\n",
        "from tensorflow.compat.v1.keras.losses import mse, binary_crossentropy\n",
        "from tensorflow.compat.v1.keras.utils import plot_model\n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = (n - 1) * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 50\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # help_ = \"Load h5 model trained weights\"\n",
        "    # parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
        "    # help_ = \"Use mse loss instead of binary cross entropy (default)\"\n",
        "    # parser.add_argument(\"-m\",\n",
        "    #                     \"--mse\",\n",
        "    #                     help=help_, action='store_true')\n",
        "    # args = parser.parse_args()\n",
        "    models = (encoder, decoder)\n",
        "    data = (x_test, y_test)\n",
        "\n",
        "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
        "    # if args.mse:\n",
        "    #     reconstruction_loss = mse(inputs, outputs)\n",
        "    # else:\n",
        "    #     reconstruction_loss = binary_crossentropy(inputs,\n",
        "    #                                               outputs)\n",
        "    reconstruction_loss = binary_crossentropy(inputs,\n",
        "                                                  outputs)\n",
        " \n",
        "\n",
        "    reconstruction_loss *= original_dim\n",
        "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "    kl_loss = K.sum(kl_loss, axis=-1)\n",
        "    kl_loss *= -0.5\n",
        "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "    vae.add_loss(vae_loss)\n",
        "    vae.compile(optimizer='adam')\n",
        "    vae.summary()\n",
        "    plot_model(vae,\n",
        "               to_file='vae_mlp.png',\n",
        "               show_shapes=True)\n",
        "\n",
        "    # if args.weights:\n",
        "    #     vae.load_weights(args.weights)\n",
        "    # else:\n",
        "    #     # train the autoencoder\n",
        "    #     vae.fit(x_train,\n",
        "    #             epochs=epochs,\n",
        "    #             batch_size=batch_size,\n",
        "    #             validation_data=(x_test, None))\n",
        "    #     vae.save_weights('vae_mlp_mnist.h5')\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
        "\n",
        "    %tensorboard --logdir logs/fit\n",
        "\n",
        "\n",
        "# train the autoencoder\n",
        "    vae.fit(x_train,\n",
        "           epochs=epochs,\n",
        "           batch_size=batch_size,\n",
        "           validation_data=(x_test, None),\n",
        "           callbacks=[tensorboard_callback])\n",
        "    vae.save_weights('vae_mlp_mnist.h5')\n",
        "\n",
        "\n",
        "    plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=batch_size,\n",
        "                 model_name=\"vae_mlp\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}